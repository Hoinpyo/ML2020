{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "93b41229599509f643f1c1df3b86b3f79b66666f"
   },
   "source": [
    "<font color=\"#CC3D3D\"><p>\n",
    "# Build models (ver.1027)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "93b41229599509f643f1c1df3b86b3f79b66666f"
   },
   "source": [
    "- `INPUT`: 학습용(`X_train.csv`, `y_train`)과 평가용(`X_test.csv`) 데이터 \n",
    "- `OUTPUT`: 위 데이터를 이용하여 구축한 모형이 생성한 예측결과(`submission.csv`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Data Wrangling\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib import font_manager, rc\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# EDA\n",
    "import klib\n",
    "\n",
    "# Preprocessing & Feature Engineering\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Modeling\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import ClassifierMixin\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Utility\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import Image\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import platform\n",
    "from itertools import combinations\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skfold = StratifiedKFold(n_splits=5) # 교차검증 시 남녀비율을 맞추기 위해 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dd7e3527fd56e5b5855c6846ca226f5f860f725b"
   },
   "source": [
    "### 1. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "794123064b09797f51add356a676fb6a7aed015d"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'y_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7d974cd59862>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# 학습용 정답 데이터를 읽는다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'y_train.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cp949'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgender\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# 전처리를 동일하게 적용하기 위해 두 데이터를 합한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1872\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1873\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1874\u001b[1;33m                 \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1875\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1876\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'y_train.csv'"
     ]
    }
   ],
   "source": [
    "# 학습용과 평가용(제출용) 데이터를 읽어들인다.\n",
    "train = pd.read_csv(os.path.abspath(\"../dat\")+'/X_train.csv', encoding='cp949')\n",
    "test = pd.read_csv(os.path.abspath(\"../dat\")+'/X_test.csv', encoding='cp949')\n",
    "\n",
    "# 분석에 필요없는 ID 필드를 데이터에서 제거하고, 전처리 후 학습용과 제출용 데이터를 분리하기 위해 ID는 보관한다.\n",
    "train_id = train['cust_id']\n",
    "test_id = test['cust_id']\n",
    "del train['cust_id'], test['cust_id']\n",
    "\n",
    "# 학습용 정답 데이터를 읽는다.\n",
    "y_train = pd.read_csv('y_train.csv', encoding='cp949').gender\n",
    "\n",
    "# 전처리를 동일하게 적용하기 위해 두 데이터를 합한다.\n",
    "features = pd.concat([train, test]).reset_index(drop=True)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용하지 않을 feature 삭제\n",
    "del features['겨울-구매비율']\n",
    "del features['여름-구매비율']\n",
    "del features['발렌타인']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Cleansing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 변수와 수치형 변수를 분리\n",
    "cat_features = features.select_dtypes(include=['object']).columns.to_list()\n",
    "num_features = features.select_dtypes(exclude='object').columns.to_list()\n",
    "#num_features = [c for c in features.columns.tolist() if c not in cat_features]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Impute Missing Values*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측값 처리\n",
    "features[num_features] = features[num_features].fillna(0)\n",
    "features[cat_features] = SimpleImputer(strategy=\"most_frequent\").fit_transform(features[cat_features])\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Deal with Outliers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치형 변수에 대해 이상치(outlier)를 처리\n",
    "features[num_features] = features[num_features].apply(lambda x: x.clip(x.quantile(.05), x.quantile(.95)), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Transform Features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치형 변수를 정규분포에 가깝게 만들기\n",
    "features[num_features] = PowerTransformer(standardize=True).fit_transform(features[num_features])\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Encode Categorical Variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 변수에 One-Hot-Encoding 후 수치형 변수와 병합\n",
    "if len(cat_features) > 0:\n",
    "    features = pd.concat([features[num_features], pd.get_dummies(features[cat_features])], axis=1)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Select Features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1차 그룹과제 1등 팀의 모델에 사용된 feature를 사용했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features[['총구매액', '구매건수', '평균구매액', '최대구매액', '구매상품종류1', '내점일수', '구매주기', '봄-구매비율', '가을-구매비율', '환불금액', '환불건수', '내점당구매액', '내점당구매건수', '구매 추세 기울기', '화장품구매주기', '식료품구매액비율', '평균식료품구매액분위', '고가상품구매율', '베스트셀러구매비율', '환불비율', '스타킹구매건수', '평일방문비율', '평균대비구매비중', '남성포함상품구매건수', '여성키워드상품구매건수', '다양한 매장 방문', '가격선호도', '선호방문월', '행사상품구매수', '저가상품구매율', '요일 간 구매건수 편차', '충동지수', '남성용품구매건수', '여성용품구매건수', '하루 최고 구매 건수', '만족도 떨어지는 제품 총 구입금액', '만족도 떨어지는 제품 총 구매건수', '가성비 제품 총 구입금액', '휴면일수', '아동용품 구매건수', '명절식품비용', '단독상품군구매율', '구매액표준편차', '중복구매배수', '의류_거래율(의류+식품+생활잡화)', '패션_대비_남성상품_거래율', '패션_대비_여성상품_거래율', '전체방문_중_환불방문(표준화)', '월_재방문율', '시즌 마감 방문비율', '상반기 구매비율', '구매 추세', '주구매지점재방문률', '내점시마다 구매액의 일관성', '주구매상품_가공식품', '주구매상품_건강식품', '주구매상품_골프', '주구매상품_구두', '주구매상품_기타', '주구매상품_남성 캐주얼', '주구매상품_남성정장', '주구매상품_농산물', '주구매상품_디자이너', '주구매상품_생활잡화', '주구매상품_섬유잡화', '주구매상품_셔츠', '주구매상품_스포츠', '주구매상품_시티웨어', '주구매상품_식기', '주구매상품_악기', '주구매상품_일용잡화', '주구매상품_주류', '주구매상품_주방가전', '주구매상품_축산가공', '주구매상품_커리어', '주구매상품_트래디셔널', '주구매상품_화장품', '주구매지점_강남점', '주구매지점_노원점', '주구매지점_대구점', '주구매지점_본  점', '주구매지점_부산본점', '주구매지점_안양점', '주구매지점_전주점', '고객등급_1등급고객', '고객등급_2등급고객', '고객등급_3등급고객', '고객등급_vip고객', '고객등급_관심대상고객', '고객등급_수면고객', '고객등급_신규고객', '주환불상품_L/C골프의류', '주환불상품_N.B', '주환불상품_건강가전', '주환불상품_국산A/V', '주환불상품_국산ACC', '주환불상품_넥타이(특정)', '주환불상품_니  트', '주환불상품_단기행사', '주환불상품_디자이너부틱', '주환불상품_디자이너캐릭터', '주환불상품_란제리행사', '주환불상품_명품남성', '주환불상품_명품잡화', '주환불상품_상품군미지정', '주환불상품_수입도자기', '주환불상품_수입의류', '주환불상품_수입주방', '주환불상품_숙녀단기행사', '주환불상품_스카프(단기행사)', '주환불상품_스타킹(특정)', '주환불상품_시니어', '주환불상품_시티정장바지', '주환불상품_식탁', '주환불상품_아웃도아', '주환불상품_어덜트', '주환불상품_영 캐릭터', '주환불상품_영캐주얼', '주환불상품_영플라자(영캐주얼)', '주환불상품_위생세제', '주환불상품_유니섹스캐주얼', '주환불상품_일반핸드백', '주환불상품_장갑(특정)', '주환불상품_직수입', '주환불상품_캐릭터 여화', '주환불상품_토들러', '주환불상품_트래디셔널', '주환불상품_피혁', '라이프 스타일(평균)_남성의류', '라이프 스타일(평균)_명품', '라이프 스타일(평균)_생활용품', '라이프 스타일(평균)_식품', '라이프 스타일(평균)_여성의류', '라이프 스타일(평균)_예체능', '라이프 스타일(평균)_화장품', '라이프 스타일(건수)_가전', '라이프 스타일(건수)_기타', '라이프 스타일(건수)_남성의류', '라이프 스타일(건수)_명품', '라이프 스타일(건수)_식품', '라이프 스타일(건수)_예체능', '라이프 스타일(건수)_의류', '라이프 스타일(건수)_화장품', '주구매 요일_금', '주구매 요일_토', '선호방문계절_fall', '선호방문계절_spring', '선호방문계절_summer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용과 제출용 데이터로 분리\n",
    "features = pd.concat([pd.concat([train_id, test_id]).reset_index(drop=True), features], axis=1)\n",
    "X_train = features.query('cust_id in @train_id').drop('cust_id', axis=1)\n",
    "X_test = features.query('cust_id in @test_id').drop('cust_id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Tuning (Hyperparameter Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습데이터 70%, 평가데이터 30%로 데이터 분할\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.3, stratify=y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 모델 : RF, LR, XGB, GB, SVC, MLP\n",
    "# 각각의 모델의 성능을 더 올리기 위해 랜덤서치와 그리드 서치를 이용해서 튜닝\n",
    "# 튜닝의 결과를 모델에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = [\n",
    "    (\n",
    "        MLPClassifier(random_state=0),\n",
    "        {'alpha': [0.0001], \n",
    "            'batch_size': ['auto'],\n",
    "            'learning_rate' : ['adaptive'],\n",
    "            'activation': ['relu'],\n",
    "            'solver': ['sgd']}\n",
    "    ),\n",
    "    (\n",
    "        LogisticRegression(random_state=0),  \n",
    "        {'C': [0.1],      \n",
    "         'penalty': ['l2']}\n",
    "    ),\n",
    "    (\n",
    "        RandomForestClassifier(random_state=0),\n",
    "        {'n_estimators': [371],\n",
    "         'max_depth': [8]}\n",
    "        # 'max_features': (np.arange(0.5, 1.0, 0.1)*X_train.shape[1]).astype(int)}\n",
    "    ),\n",
    "    (\n",
    "        GradientBoostingClassifier(random_state=0),\n",
    "        {'n_estimators': [250],\n",
    "         'learning_rate': [0.060000000000000005],\n",
    "         'max_depth': [1]}\n",
    "    ),\n",
    "    (\n",
    "        XGBClassifier(random_state=0),\n",
    "        {'n_estimators': [126],\n",
    "         'learning_rate': [0.04],\n",
    "         'max_depth': [3]}\n",
    "    ),\n",
    "    (\n",
    "        SVC(probability=True),\n",
    "       {'kernel': ['linear','rbf']}\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "clfs_tuned = []  # 튜닝된 모델을 저장\n",
    "for clf, param_grid in tqdm(clfs):\n",
    "    start = time.time()\n",
    "    rand_search = RandomizedSearchCV(clf, param_grid, n_iter=20, scoring='roc_auc', \n",
    "                                     cv=skfold, random_state=0, n_jobs=-1)\n",
    "    rand_search.fit(X_train, y_train)\n",
    "    clf_name = type(clf).__name__\n",
    "    clf_score = rand_search.score(X_dev, y_dev)\n",
    "    print('{:30s} {:30f} {:.1f}'.format(clf_name, clf_score, time.time() - start))\n",
    "    clfs_tuned.append((clf_name, rand_search, clf_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Correlation between models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = []\n",
    "for name, clf, clf_score in clfs_tuned:\n",
    "    pred = clf.predict_proba(X_dev)[:,1]\n",
    "    name = f'{name} \\n({clf_score:.4f})'\n",
    "    pred_results.append(pd.Series(pred, name=name))\n",
    "ensemble_results = pd.concat(pred_results, axis=1)\n",
    "\n",
    "# 모형의 예측값 간의 상관관계를 보기 위해 hitmap을 도식한다.\n",
    "plt.figure(figsize = (8,6))\n",
    "g = sns.heatmap(ensemble_results.corr(), annot=True, cmap='Blues')\n",
    "g.set_title(\"Correlation between models\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Mean agreement vs. Performance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean agreement\n",
    "(ensemble_results.corr().sum()-1)/(ensemble_results.corr().shape[0]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corr = (ensemble_results.corr().sum()-1)/(ensemble_results.corr().shape[0]-1)\n",
    "names = corr.index\n",
    "aucs = np.array(corr.index.str[-7:-1]).astype(float)\n",
    "df = pd.DataFrame({'model': names, 'auc': aucs, 'cor': corr})        \n",
    "\n",
    "plt.figure(figsize=(15,12))\n",
    "g = sns.scatterplot(x=\"cor\", y=\"auc\", data=df, s=40, color='red')\n",
    "for line in range(0, df.shape[0]):\n",
    "     g.text(df.cor[line]+0.003, df.auc[line]-0.003, \n",
    "            df.model[line], horizontalalignment='left', \n",
    "            size='medium', color='black', weight='semibold')\n",
    "        \n",
    "plt.xlim((df.cor.min()-0.01,df.cor.max()+0.01))\n",
    "plt.ylim((df.auc.min()-0.01,df.auc.max()+0.01))\n",
    "plt.xlabel('Mean Agreement')\n",
    "plt.ylabel('ROC-AUC')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Averaging Ensemble*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging 앙상블에 사용하지 않을 모델은 주석 처리하시오.\n",
    "selected = [ \n",
    "    'MLPClassifier',\n",
    "    'LogisticRegression', \n",
    "    'RandomForestClassifier', \n",
    "    'GradientBoostingClassifier',\n",
    "    'XGBClassifier',\n",
    "    'SVC'\n",
    "]\n",
    "models_for_ensemble = [clf for clf in clfs_tuned if clf[0] in selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = 0\n",
    "for p in tqdm([0, 1, 2.56]):  # p==1:산술평균, p=0:기하평균, 그 외:멱평균(주의:멱평균은 과적합 가능성이 높음)    \n",
    "    for i in range(3, len(models_for_ensemble)+1): # 3개 이상의 모델이 앙상블된 것들 중 가장 성능이 좋은 것을 선택\n",
    "        for models in combinations(models_for_ensemble, i):\n",
    "            if p == 0:\n",
    "                pred_mean = gmean([clf.predict_proba(X_dev)[:,1] for name, clf, _ in models], axis=0)\n",
    "            else:\n",
    "                preds = [clf.predict_proba(X_dev)[:,1] for name, clf, _ in models]\n",
    "                pred_mean = (np.sum(np.array(preds)**p, axis=0) / len(models))**(1/p)\n",
    "            score = roc_auc_score(y_dev, pred_mean)\n",
    "            if max_score < score:\n",
    "                best_avg_ensemble = (p, models, score)\n",
    "                max_score = score\n",
    "\n",
    "p, models, score = best_avg_ensemble\n",
    "print('p={}\\n{}\\n{}'.format(p, '●'.join([clf_name for clf_name, _, _ in models]), score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn의 다른 classifier와 호환성을 갖기위해 Custom Classifier인 \"AveragingClassifier\" 생성\n",
    "\n",
    "class AveragingClassifier(ClassifierMixin):\n",
    "    def __init__(self, estimators, p):\n",
    "        self.estimators = estimators\n",
    "        self.p = p\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        return None\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.p == 0:\n",
    "            pred = gmean([clf.predict(X) for name, clf in estimators], axis=0)\n",
    "        else:\n",
    "            preds = [clf.predict(X) for name, clf in estimators]\n",
    "            pred = (np.sum(np.array(preds)**self.p, axis=0) / len(estimators))**(1/self.p)\n",
    "        return pred\n",
    "         \n",
    "    def predict_proba(self, X):\n",
    "        if self.p == 0:\n",
    "            prob = gmean([clf.predict_proba(X) for name, clf in estimators], axis=0)\n",
    "        else:\n",
    "            probs = [clf.predict_proba(X) for name, clf in estimators]\n",
    "            prob = (np.sum(np.array(probs)**self.p, axis=0) / len(estimators))**(1/self.p)\n",
    "        return prob\n",
    "    \n",
    "estimators = [(name, clf) for name, clf, _ in models]\n",
    "avg_clf = AveragingClassifier(estimators, p)\n",
    "avg_clf.fit(X_train, y_train)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging ensemble을 적용한 submission\n",
    "pd.DataFrame({'cust_id': test_id, 'gender': avg_clf.predict_proba(X_test)[:,1]}).to_csv('os.path.abspath(\"../dat\")+'/조기흠-submission-features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "db6a418af5947fb6d1546dc8949ac55663504f18"
   },
   "source": [
    "<font color=\"#CC3D3D\"><p>\n",
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
